{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sk learn classes\n",
    "\n",
    "Clases para generar un pipeline de sk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import json\n",
    "from functools import reduce\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "transformer object that turns the data json into arrays of tokenized words.\n",
    "'''\n",
    "\n",
    "class JsonTransform():\n",
    "    \n",
    "    '''\n",
    "    add category determines if tag category is added to the final arrays.\n",
    "    '''\n",
    "    def __init__(self,add_category=False):\n",
    "        self.add_category = add_category\n",
    "        \n",
    "    '''\n",
    "    returns arrays of word arrays from tags\n",
    "    '''    \n",
    "\n",
    "    \n",
    "    def fit(self,X=None,y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self,X):\n",
    "        return [ self.tag_array_to_word_array(instance) for instance in self.data_to_tags(data)]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    returns tag as array of words (no ':').\n",
    "    add_category determines if the tag category is added to the array.\n",
    "    '''\n",
    "    def tag_to_words(self,tag):\n",
    "        category, text = tag.split(\":\")\n",
    "        if self.add_category:\n",
    "            return word_tokenize(\"{} {}\".format(category,text).lower())\n",
    "        return word_tokenize(text.lower())\n",
    "\n",
    "    '''\n",
    "    transforms array of tags into array of words.\n",
    "    add_category determines if the tag categories are added to the text.\n",
    "    '''\n",
    "    def tag_array_to_word_array(self,tags):\n",
    "        aux_tags = list(tags)\n",
    "        aux_tags[0] = self.tag_to_words(tags[0])\n",
    "        return reduce(lambda x,y : x + self.tag_to_words(y),aux_tags)\n",
    "\n",
    "    '''\n",
    "    returns array of tag arrays from original json.\n",
    "    '''\n",
    "    def data_to_tags(self,data):\n",
    "        return [instance[\"tags\"] for instance in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "#constants\n",
    "DATA_PATH=\"data.json\"\n",
    "data = json.load(open(DATA_PATH))\n",
    "json_transformer = JsonTransform(False)\n",
    "documents = json_transformer.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Glove Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculates embedding, mapping a tokenized document to a vector.\n",
    "To calculate the vector we use a weighted average of precomputed Glove Vectors. \n",
    "Weights of the average are given by TFIDF scores.\n",
    "'''\n",
    "\n",
    "class TfIdfGloveTransformer():\n",
    "    \n",
    "    '''\n",
    "    word_embedder is pretrained gensim.KeyedVectors model\n",
    "    \n",
    "    dim is the dimension on word_embedder\n",
    "    '''\n",
    "    def __init__(self,word_embedder,dim=300):\n",
    "        self.word_embedder = word_embedder\n",
    "        self.dim=dim\n",
    "        self.word_dict = corpora.Dictionary(documents,prune_at=None)\n",
    "        self.bows = None\n",
    "        self.tfidf = None\n",
    "        self.token2id = None\n",
    "        \n",
    "    '''\n",
    "    Fits from corpus of tokenized documents.\n",
    "    '''\n",
    "    def fit(self,X,y=None):\n",
    "        self.bows = [self.word_dict.doc2bow(doc) for doc in X]\n",
    "        self.tfidf = TfidfModel(self.bows,normalize=True)\n",
    "        self.token2id = self.word_dict.token2id\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    returns embedding representation of documents in X\n",
    "    '''\n",
    "    \n",
    "    def transform(self,X):\n",
    "        new_bows = [self.word_dict.doc2bow(doc) for doc in X]\n",
    "        result = np.zeros((len(X),self.dim))\n",
    "        # perhaps this can be implemented better in a vectorial way\n",
    "        for i, (doc,bow) in enumerate(zip(X,new_bows)):\n",
    "            score_hash = { tup[0]:tup[1] for tup in self.tfidf.__getitem__(bow,-1)} # threshold\n",
    "            weighted_embeddings = np.array([np.dot(model[word],score_hash[self.token2id[word]]) if word in model else np.zeros((1,self.dim)) for word in doc])\n",
    "            result[i] = np.sum(weighted_embeddings, axis=0)\n",
    "        return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# constants\n",
    "GLOVE_PATH= \"glove-sbwc.i25.vec\"\n",
    "DATA_PATH=\"data.json\"\n",
    "vectors = 855380\n",
    "model=KeyedVectors.load_word2vec_format(GLOVE_PATH,limit=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-121e7ca4629d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mjson_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJsonTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfIdfGloveTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "data = json.load(open(DATA_PATH))\n",
    "json_transformer = JsonTransform(False)\n",
    "documents = json_transformer.transform(data)\n",
    "tfidf = TfIdfGloveTransformer(model).fit(documents)\n",
    "result = tfidf.transform(documents)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf LDA Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel, LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates doc embeddings baed on topic modelling.\n",
    "Does Tf-Idf transformation and then computes probability distibutions with LDA algorithm.\n",
    "\"\"\"\n",
    "\n",
    "class LdaTransformer():\n",
    "    \"\"\"\n",
    "    dim: amount of topics to model. aka output vector dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self,dim=20):\n",
    "        self.dim=dim\n",
    "        self.word_dict = corpora.Dictionary(documents,prune_at=None)\n",
    "        self.bows = None\n",
    "        self.tfidf = None\n",
    "        self.token2id = None\n",
    "        self.lda = None\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        self.bows = [self.word_dict.doc2bow(doc) for doc in X]\n",
    "        self.tfidf = TfidfModel(self.bows,normalize=True)\n",
    "        self.token2id = self.word_dict.token2id\n",
    "        self.lda = LdaModel(self.tfidf[self.bows],num_topics=self.dim,minimum_probability=0)\n",
    "        return self\n",
    "    \n",
    "    \"\"\"\n",
    "    receives tokenized documents and returns the distribution of each.\n",
    "    \"\"\"\n",
    "    def transform(self,X):\n",
    "        new_bows = [self.word_dict.doc2bow(doc) for doc in X]\n",
    "        distributions = np.array(self.lda[self.tfidf[new_bows]])\n",
    "        return np.reshape(np.delete(distributions,np.s_[:1],2),(len(X),self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "DATA_PATH=\"data.json\"\n",
    "\n",
    "data = json.load(open(DATA_PATH))\n",
    "json_transformer = JsonTransform(False)\n",
    "documents = json_transformer.transform(data)\n",
    "lda = LdaTransformer().fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNeighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper of sklearn balltree to put in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "wrapper for sklearn BallTree that can be added to a pipeline\n",
    "'''\n",
    "\n",
    "class BallTreePredictor():\n",
    "    \n",
    "    def __init__(self,k=5):\n",
    "        self.tree = None\n",
    "        self.k=k\n",
    "        \n",
    "    def set_neighbors(self,k):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        self.tree = BallTree(X)\n",
    "        return self\n",
    "        \n",
    "    def predict(self,X):\n",
    "        return self.tree.query(X,self.k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 7.66705467],\n",
       "        [0.        , 8.68959395],\n",
       "        [0.        , 7.78825885],\n",
       "        [0.        , 7.66705467],\n",
       "        [0.        , 7.78825885]]), array([[0, 3],\n",
       "        [1, 3],\n",
       "        [2, 4],\n",
       "        [3, 0],\n",
       "        [4, 2]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "tree = BallTreePredictor(k=2).fit(result)\n",
    "tree.predict(result[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4.36829008e-09, 6.04630833e-02, 1.08262327e+00, 1.10715184e+00,\n",
       "         1.12590370e+00],\n",
       "        [5.96046448e-08, 1.08262331e+00, 1.11142600e+00, 1.12710288e+00,\n",
       "         1.13011397e+00],\n",
       "        [4.05954099e-09, 6.04630832e-02, 1.12710284e+00, 1.15077592e+00,\n",
       "         1.16889645e+00],\n",
       "        [9.31322575e-10, 2.50989367e-02, 1.10715185e+00, 1.11142595e+00,\n",
       "         1.15077592e+00],\n",
       "        [4.26785619e-09, 2.50989376e-02, 1.12590370e+00, 1.13011393e+00,\n",
       "         1.16889645e+00]]), array([[0, 2, 1, 3, 4],\n",
       "        [1, 0, 3, 2, 4],\n",
       "        [2, 0, 1, 3, 4],\n",
       "        [3, 4, 0, 1, 2],\n",
       "        [4, 3, 0, 1, 2]]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([(\"json\",JsonTransform()),(\"embedder\",LdaTransformer()),(\"tree\",BallTreePredictor())])\n",
    "pipe.fit(data) # fit and predict directly on json files\n",
    "pipe.predict(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
